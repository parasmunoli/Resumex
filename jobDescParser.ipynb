{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-20T12:43:45.257906Z",
     "start_time": "2025-03-20T12:43:40.861517Z"
    }
   },
   "source": [
    "import torch\n",
    "import transformers\n",
    "from huggingface_hub import login\n",
    "import os\n",
    "import dotenv"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T12:43:46.986718Z",
     "start_time": "2025-03-20T12:43:46.714953Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dotenv.load_dotenv()\n",
    "HF_API_KEY = os.getenv('hfAccessToken')\n",
    "login(HF_API_KEY)"
   ],
   "id": "ea63e74ce646a8bb",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T12:47:09.470797Z",
     "start_time": "2025-03-20T12:43:48.229717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\", model=model_id, model_kwargs={\"torch_dtype\": torch.bfloat16}, device_map=\"auto\"\n",
    ")\n",
    "pipeline(\"Hello job description parser\")"
   ],
   "id": "b970cba0f0e09175",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "796931b74c5f4e3b92268a240ff76781"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello job description parser? I am looking for a job. I can write a program that can parse job descriptions to identify the'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T13:25:45.325118Z",
     "start_time": "2025-03-20T13:25:43.030428Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset_path = \"llama_finetune_data.json\"\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "# Verify data format\n",
    "print(dataset[0])\n"
   ],
   "id": "d1acc87af201a22b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "32ac1081a1894a878028b2a39cce0fd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Extract key points from the given job description.', 'input': 'We are looking for hire experts flutter developer. So you are eligible this post then apply your resume.\\nJob Types: Full-time, Part-time\\nSalary: ₹20,000.00 - ₹40,000.00 per month\\nBenefits:\\nFlexible schedule\\nFood allowance\\nSchedule:\\nDay shift\\nSupplemental Pay:\\nJoining bonus\\nOvertime pay\\nExperience:\\ntotal work: 1 year (Preferred)\\nHousing rent subsidy:\\nYes\\nIndustry:\\nSoftware Development\\nWork Remotely:\\nTemporarily due to COVID-19', 'output': \"{'Role Overview': 'we are looking for hire experts flutter developer  so you are eligible this post then apply your resume', 'Key Responsibilities': [], 'Qualifications & Skills': ['flutter']}\"}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-20T13:30:52.247849Z",
     "start_time": "2025-03-20T13:30:51.672098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Enables 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "# Load model with offloading\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n"
   ],
   "id": "fc7ad9c55f8e66b9",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[9]\u001B[39m\u001B[32m, line 14\u001B[39m\n\u001B[32m      7\u001B[39m bnb_config = BitsAndBytesConfig(\n\u001B[32m      8\u001B[39m     load_in_4bit=\u001B[38;5;28;01mTrue\u001B[39;00m,  \u001B[38;5;66;03m# Enables 4-bit quantization\u001B[39;00m\n\u001B[32m      9\u001B[39m     bnb_4bit_compute_dtype=torch.float16,\n\u001B[32m     10\u001B[39m     bnb_4bit_use_double_quant=\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[32m     11\u001B[39m )\n\u001B[32m     13\u001B[39m \u001B[38;5;66;03m# Load model with offloading\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m14\u001B[39m model = \u001B[43mAutoModelForCausalLM\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m    \u001B[49m\u001B[43mquantization_config\u001B[49m\u001B[43m=\u001B[49m\u001B[43mbnb_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mauto\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\n\u001B[32m     18\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[38;5;66;03m# Load tokenizer\u001B[39;00m\n\u001B[32m     21\u001B[39m tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Study\\Projects\\Resumex\\.venv\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:564\u001B[39m, in \u001B[36m_BaseAutoModelClass.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[39m\n\u001B[32m    562\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(config) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mcls\u001B[39m._model_mapping.keys():\n\u001B[32m    563\u001B[39m     model_class = _get_model_class(config, \u001B[38;5;28mcls\u001B[39m._model_mapping)\n\u001B[32m--> \u001B[39m\u001B[32m564\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mmodel_class\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_pretrained\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    565\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpretrained_model_name_or_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43mmodel_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m=\u001B[49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mhub_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    566\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    567\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    568\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mUnrecognized configuration class \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mconfig.\u001B[34m__class__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m for this kind of AutoModel: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mcls\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m\n\u001B[32m    569\u001B[39m     \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mModel type should be one of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[33m'\u001B[39m\u001B[33m, \u001B[39m\u001B[33m'\u001B[39m.join(c.\u001B[34m__name__\u001B[39m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mfor\u001B[39;00m\u001B[38;5;250m \u001B[39mc\u001B[38;5;250m \u001B[39m\u001B[38;5;129;01min\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28mcls\u001B[39m._model_mapping.keys())\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    570\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Study\\Projects\\Resumex\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:262\u001B[39m, in \u001B[36mrestore_default_torch_dtype.<locals>._wrapper\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    260\u001B[39m old_dtype = torch.get_default_dtype()\n\u001B[32m    261\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m262\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    263\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    264\u001B[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Study\\Projects\\Resumex\\.venv\\Lib\\site-packages\\transformers\\modeling_utils.py:4262\u001B[39m, in \u001B[36mPreTrainedModel.from_pretrained\u001B[39m\u001B[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001B[39m\n\u001B[32m   4259\u001B[39m     device_map = infer_auto_device_map(model, dtype=target_dtype, **device_map_kwargs)\n\u001B[32m   4261\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m hf_quantizer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m4262\u001B[39m         \u001B[43mhf_quantizer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mvalidate_environment\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice_map\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   4264\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m device_map \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   4265\u001B[39m     model.tie_weights()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mC:\\Study\\Projects\\Resumex\\.venv\\Lib\\site-packages\\transformers\\quantizers\\quantizer_bnb_4bit.py:103\u001B[39m, in \u001B[36mBnb4BitHfQuantizer.validate_environment\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    101\u001B[39m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m    102\u001B[39m     \u001B[38;5;28;01melif\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mcpu\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map_without_lm_head.values() \u001B[38;5;129;01mor\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33mdisk\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m device_map_without_lm_head.values():\n\u001B[32m--> \u001B[39m\u001B[32m103\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    104\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mSome modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    105\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mquantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    106\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33min 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    107\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33m`from_pretrained`. Check \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    108\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mhttps://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    109\u001B[39m             \u001B[33m\"\u001B[39m\u001B[33mfor more details. \u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    110\u001B[39m         )\n\u001B[32m    112\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m version.parse(importlib.metadata.version(\u001B[33m\"\u001B[39m\u001B[33mbitsandbytes\u001B[39m\u001B[33m\"\u001B[39m)) < version.parse(\u001B[33m\"\u001B[39m\u001B[33m0.39.0\u001B[39m\u001B[33m\"\u001B[39m):\n\u001B[32m    113\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m    114\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mYou have a version of `bitsandbytes` that is not compatible with 4bit inference and training\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    115\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m make sure you have the latest version of `bitsandbytes` installed\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    116\u001B[39m     )\n",
      "\u001B[31mValueError\u001B[39m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a0a34d98e34eb56a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
